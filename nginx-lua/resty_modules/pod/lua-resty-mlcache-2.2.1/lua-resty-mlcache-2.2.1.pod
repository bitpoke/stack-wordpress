=encoding utf-8


=head1 lua-resty-mlcache


[![Build Status][badge-travis-image]][badge-travis-url]

Fast and automated multi-level cache for OpenResty.

This library can be manipulated as a key/value store caching scalar Lua
types and tables, but is built on top of [lua_shared_dict] and
[lua-resty-lrucache]. This combination allows for extremely performant and
flexible caching.

Features:


=over


=item *

Caching and negative caching with TTLs.

=item *

Built-in mutex via [lua-resty-lock] to prevent dog-pile effects to your
database/backend on cache misses.

=item *

Built-in inter-workers communication to propagate cache invalidations
and allow workers to update their L1 (lua-resty-lrucache) caches upon changes
(C<set()>, C<delete()>).

=item *

Multiple isolated instances can be created to hold various types of data
while relying on the I<same> C<lua_shared_dict> L2 cache.


=back

Illustration of the various caching levels built into this library:


    ┌─────────────────────────────────────────────────┐
    │ Nginx                                           │
    │       ┌───────────┐ ┌───────────┐ ┌───────────┐ │
    │       │worker     │ │worker     │ │worker     │ │
    │ L1    │           │ │           │ │           │ │
    │       │ Lua cache │ │ Lua cache │ │ Lua cache │ │
    │       └───────────┘ └───────────┘ └───────────┘ │
    │             │             │             │       │
    │             ▼             ▼             ▼       │
    │       ┌───────────────────────────────────────┐ │
    │       │                                       │ │
    │ L2    │           lua_shared_dict             │ │
    │       │                                       │ │
    │       └───────────────────────────────────────┘ │
    │                           │                     │
    │                           ▼                     │
    │                  ┌──────────────────┐           │
    │                  │     callback     │           │
    │                  └────────┬─────────┘           │
    └───────────────────────────┼─────────────────────┘
                                │
      L3                        │   I/O fetch
                                ▼
    
                       Database, API, I/O...

The cache level hierarchy is:

=over


=item *

B<L1>: Least-Recently-Used Lua-land cache using [lua-resty-lrucache].
Provides the fastest lookup if populated, and avoids exhausting the workers'
Lua VM memory.

=item *

B<L2>: C<lua_shared_dict> memory zone shared by all workers. This level
is only accessed if L1 was a miss, and prevents workers from requesting the
L3 cache.

=item *

B<L3>: a custom function that will only be run by a single worker
to avoid the dog-pile effect on your database/backend
(via [lua-resty-lock]). Values fetched via L3 will be set to the L2 cache
for other workers to access.


=back


=head1 Synopsis



    # nginx.conf
    
    http {
        # you do not need to configure the following line when you
        # use LuaRocks or opm.
        lua_package_path "/path/to/lua-resty-mlcache/lib/?.lua;;";
    
        lua_shared_dict cache_dict 1m;
    
        init_by_lua_block {
            local mlcache = require "resty.mlcache"
    
            local cache, err = mlcache.new("my_cache", "cache_dict", {
                lru_size = 500,    -- size of the L1 (Lua-land LRU) cache
                ttl      = 3600,   -- 1h ttl for hits
                neg_ttl  = 30,     -- 30s ttl for misses
            })
            if err then
    
            end
    
            -- we put our instance in the global table for brivety in
            -- this example, but prefer an upvalue to one of your modules
            -- as recommended by ngx_lua
            _G.cache = cache
        }
    
        server {
            listen 8080;
    
            location / {
                content_by_lua_block {
                    local function callback(username)
                        -- this only runs *once* until the key expires, so
                        -- do expensive operations like connecting to a remote
                        -- backend here. i.e: call a MySQL server in this callback
                        return db:get_user(username) -- { name = "John Doe", email = "john@example.com" }
                    end
    
                    -- this call will respectively hit L1 and L2 before running the
                    -- callback (L3). The returned value will then be stored in L2 and
                    -- L1 for the next request.
                    local user, err = cache:get("my_key", nil, callback, "John Doe")
                    if err then
    
                    end
    
                    ngx.say(user.username) -- "John Doe"
                }
            }
        }
    }




=head1 Requirements



=over


=item *

OpenResty E<gt>= C<1.11.2.2>

=over


=item *

ngx_lua

=item *

lua-resty-lrucache

=item *

lua-resty-lock


=back


=back

This library B<should> be entirely compatible with older versions of
OpenResty.

| OpenResty   | Compatibility
|------------:|:--------------------|
| E<lt>           | not tested
| C<1.11.2.2>  | :heavy_check_mark:
| C<1.11.2.3>  | :heavy_check_mark:
| C<1.11.2.4>  | :heavy_check_mark:
| C<1.11.2.5>  | :heavy_check_mark:
| C<1.13.6.1>  | :heavy_check_mark:
| C<1.13.6.2>  | :heavy_check_mark:
| E<gt>           | not tested




=head1 Installation


With L<Luarocks|https://luarocks.org/>:


    $ luarocks install lua-resty-mlcache

Or via L<opm|https://github.com/openresty/opm>:


    $ opm get thibaultcha/lua-resty-mlcache

Or manually:

Once you have a local copy of this module's C<lib/> directory, add it to your
C<LUA_PATH> (or C<lua_package_path> directive for OpenResty):


    /path/to/lib/?.lua;




=head1 Methods



=head2 new

B<syntax:> C<cache, err = mlcache.new(name, shm, opts?)>

Creates a new mlcache instance. If failed, returns C<nil> and a string
describing the error.

The first argument C<name> is an arbitrary name of your choosing for this cache,
and must be a string. Each mlcache instance namespaces the values it holds
according to its name, so several instances with the same name will
share the same data.

The second argument C<shm> is the name of the C<lua_shared_dict> shared memory
zone. Several instances of mlcache can use the same shm (values will be
namespaced).

The third argument C<opts> is optional. If provided, it must be a table
holding the desired options for this instance. The possible options are:


=over


=item *

C<lru_size>: a number defining the size of the underlying L1 cache
(lua-resty-lrucache instance). This size is the maximal number of items
that the L1 LRU cache can hold.
B<Default:> C<100>.

=item *

C<ttl>: a number specifying the expiration time period of the cached
values. The unit is seconds, but accepts fractional number parts, like
C<0.3>. A C<ttl> of C<0> means the cached values will never expire.
B<Default:> C<30>.

=item *

C<neg_ttl>: a number specifying the expiration time period of the cached
misses (when the L3 callback returns C<nil>). The unit is seconds, but
accepts fractional number parts, like C<0.3>. A C<neg_ttl> of C<0> means the
cached misses will never expire.
B<Default:> C<5>.

=item *

C<resurrect_ttl>: I<optional> number. When specified, the mlcache instance will
attempt to resurrect stale values when the L3 callback returns C<nil, err>
(soft errors). More details are available for this option in the
L<get()> section. The unit is seconds, but accepts fractional number
parts, like C<0.3>.

=item *

C<lru>: a lua-resty-lrucache instance of your choice. If specified, mlcache
will not instantiate an LRU. One can use this value to use the
C<resty.lrucache.pureffi> implementation of lua-resty-lrucache if desired.

=item *

C<shm_set_tries>: the number of tries for the lua_shared_dict C<set()>
operation. When the lua_shared_dict is full, it attempts to free up to 30
items from its queue. When the value being set is much larger than the freed
space, this option allows mlcache to retry the operation (and free more slots)
until the maximum number of tries is reached or enough memory was freed for
the value to fit.
B<Default>: C<3>.

=item *

C<shm_miss>: I<optional> string. The name of a C<lua_shared_dict>. When
specified, misses (callbacks returning C<nil>) will be cached in this separate
lua_shared_dict. This is useful to ensure that a large number of cache misses
(e.g. triggered by clients) does not evict too many cache items (hits) from
the lua_shared_dict specified by C<shm>. Particularly useful depending on the
type of workload put on mlcache.

=item *

C<resty_lock_opts>: options for [lua-resty-lock] instances. When mlcache runs
the L3 callback, it uses lua-resty-lock to ensure that a single worker runs
the provided callback.

=item *

C<shm_locks>: I<optional> string. The name of a C<lua_shared_dict>. When
specified, lua-resty-lock will use this shared dict to store its locks. This
option can help reducing cache churning: when the L2 cache (shm) is full,
every insertion (such as locks created by concurrent accesses triggering L3
callbacks) purges the oldest 30 accessed items. These purged items are most
likely to be previously (and valuable) cached values. By isolating locks in a
separate shared dict, workloads experiencing cache churning can mitigate this
effect.

=item *

C<l1_serializer>: an I<optional> function. Its signature and accepted values
are documented under the L<get()> method, along with an example.  If
specified, this function will be called by each worker every time the L1 LRU
cache is a miss and the value needs to be fetched from a lower cache level
(L2/L3).  Its purpose is to perform arbitrary serialization of the cached
item to transform it into any Lua object I<before> storing it into the L1 LRU
cache.  It can thus avoid your application from having to repeat such
transformation upon every cache hit, such as creating tables, cdata objects,
functions, etc...

=item *

C<ipc_shm>: I<optional> string. If you wish to use L<set()>,
L<delete()>, or L<purge()>, you must provide an IPC
(Inter-process communication) mechanism for workers to invalidate their L1
LRU caches. This module bundles an "off the shelf" IPC library, and you can
enable it by specifying a dedicated C<lua_shared_dict> in this option. Several
mlcache instances can use the same shared dict (events will be namespaced),
but no other actor than mlcache should tamper with it.

=item *

C<ipc>: an I<optional> table . Like the above C<ipc_shm> option, but lets you use
the IPC library of your choice to send inter-worker events.


=back

Example:


    local mlcache = require "resty.mlcache"
    
    local cache, err = mlcache.new("my_cache", "cache_shared_dict", {
        lru_size = 1000, -- hold up to 1000 items in the L1 cache (Lua VM)
        ttl      = 3600, -- caches scalar types and tables for 1h
        neg_ttl  = 60    -- caches nil values for 60s,
    })
    if not cache then
        error("could not create mlcache: " .. err)
    end

You can create several mlcache instances relying on the same underlying
C<lua_shared_dict> shared memory zone:


    local mlcache = require "mlcache"
    
    local cache_1 = mlcache.new("cache_1", "cache_shared_dict", { lru_size = 100 })
    local cache_2 = mlcache.new("cache_2", "cache_shared_dict", { lru_size = 1e5 })

In the above example, C<cache_1> is ideal for holding a few, very large values.
C<cache_2> can be used to hold a large number of small values. Both instances
will rely on the same shm: C<lua_shared_dict cache_shared_dict 2048m;>. Even if
you use identical keys in both caches, they will not conflict with each other
since they each bear a different name.

This other example instanciates an mlcache using the bundled IPC module for
inter-workers invalidation events (so we can use L<set()>,
L<delete()>, and L<purge()>):


    local mlcache = require "resty.mlcache"
    
    local cache, err = mlcache.new("my_cache_with_ipc", "cache_shared_dict", {
        lru_size = 1000,
        ipc_shm = "ipc_shared_dict"
    })




=head2 get

B<syntax:> C<value, err, hit_level = cache:get(key, opts?, callback, ...)>

Performs a cache lookup. This is the primary and most efficient method of this
module. A typical pattern is to I<not> call L<set()>, and let L<get()>
perform all the work.

When it succeeds, it returns C<value> and no error. I<>Because C<nil> values from
the L3 callback are cached to signify misses, C<value> can be nil, hence one
must rely on the second return value C<err> to determine if this method
succeeded or notI<>.

The third return value is a number which is set if no error was encountered.
It indicated the level at which the value was fetched: C<1> for L1, C<2> for L2,
and C<3> for L3.

If an error is encountered, this method returns C<nil> plus a string describing
the error.

The first argument C<key> is a string. Each value must be stored under a unique
key.

The second argument C<opts> is optional. If provided, it must be a table holding
the desired options for this key. These options will supersede the instance's
options:


=over


=item *

C<ttl>: a number specifying the expiration time period of the cached
values. The unit is seconds, but accepts fractional number parts, like
C<0.3>. A C<ttl> of C<0> means the cached values will never expire.
B<Default:> inherited from the instance.

=item *

C<neg_ttl>: a number specifying the expiration time period of the cached
misses (when the L3 callback returns C<nil>). The unit is seconds, but
accepts fractional number parts, like C<0.3>. A C<neg_ttl> of C<0> means the
cached misses will never expire.
B<Default:> inherited from the instance.

=item *

C<resurrect_ttl>: I<optional> number. When specified, C<get()> will attempt to
resurrect stale values when errors are encountered. Errors returned by the L3
callback (C<nil, err>) are considered to be failures to fetch/refresh a value.
When such return values from the callback are seen by C<get()>, and if the
stale value is still in memory, then C<get()> will resurrect the stale value
for C<resurrect_ttl> seconds. The error returned by C<get()> will be logged at
the WARN level, but I<not> returned by C<get()>.  Finally, the C<hit_level>
return value will be C<4> to signify that the served item is stale. When
C<resurrect_ttl> is reached, C<get()> will once again attempt to run the
callback. If by then, the callback returns an error again, the value is
resurrected once again, and so on. If the callback succeeds, the value is
refreshed and not marked as stale anymore. Due to current limitations within
the LRU cache module, C<hit_level> will be C<1> when stale values are upgraded
to the L1 (LRU) cache and retrieved from there.  Lua errors thrown by the
callback I<do not> trigger a resurrect, and are returned by C<get()> as usual
(C<nil, err>). When several workers time out while waiting for the worker
running the callback (e.g. because the datastore is timing out), then users
of this option will see a slight difference compared to the traditional
behavior of C<get()>. Instead of returning C<nil, err> (indicating a lock
timeout), C<get()> will return the stale value (if available), no error, and
C<hit_level> will be C<4>. However, the value will not be resurrected (since
another worker is still running the callback). The unit is seconds, but
accepts fractional number parts, like C<0.3>. This option B<must> be greater
than C<0>, to avoid stale values from being cached indefinitely.
B<Default:> inherited from the instance.

=item *

C<shm_set_tries>: the number of tries for the lua_shared_dict C<set()>
operation. When the lua_shared_dict is full, it attempts to free up to 30
items from its queue. When the value being set is much larger than the freed
space, this option allows mlcache to retry the operation (and free more slots)
until the maximum number of tries is reached or enough memory was freed for
the value to fit.
B<Default:> inherited from the instance.

=item *

C<l1_serializer>: an I<optional> function. Its signature and accepted values
are documented in the example below.
If specified, this function will be called by each worker every time the L1
LRU cache is a miss and the value needs to be fetched from a lower cache
level (L2/L3).
Its purpose is to perform arbitrary serialization of the cached item to
transform it into any Lua object I<before> storing it into the L1 LRU cache.
It can thus avoid your application from having to repeat such transformation
upon every cache hit, such as creating tables, cdata objects, functions,
etc...
B<Default:> inherited from the instance.


=back

The third argument C<callback> B<must> be a function. Its signature and return
values are documented in the following example:


    -- arg1, arg2, and arg3 are arguments forwarded to the callback from the
    -- `get()` variadic arguments, like so:
    -- cache:get(key, opts, callback, arg1, arg2, arg3)
    
    local function callback(arg1, arg2, arg3)
        -- I/O lookup logic
        -- ...
    
        -- value: the value to cache (Lua scalar or table)
        -- err: if not `nil`, will abort get(), which will return `value` and `err`
        -- ttl: ttl for this value - will override `ttl` or `neg_ttl` if specified
        return value, err, ttl
    end

This function B<can> throw Lua errors as it runs in protected mode. Such
errors thrown from the callback will be returned as strings in the second
return value C<err>.

When called, C<get()> follows the below steps:


=over


=item 1.

query the L1 cache (lua-resty-lrucache instance). This cache lives in the
Lua-land, and as such, it is the most efficient to query.

=over


=item 1.

if the L1 cache has the value, it returns the value.

=item 2.

if the L1 cache does not have the value (L1 miss), it continues.

=back


=item 2.

query the L2 cache (C<lua_shared_dict> shared memory zone). This cache is
shared by all workers, and is less efficient than the L1 cache. It also
involves serialization for Lua tables.

=over


=item 1.

if the L2 cache has the value, retrieve it.

=over


=item 1.

if C<l1_serializer> is set, run it, and set the resulting value in
the L1 cache.

=item 2.

if not, directly set the value as-is in the L1 cache.

=back


=item 2.

if the L2 cache does not have the value (L2 miss), it continues.

=back


=item 3.

creates a [lua-resty-lock], and ensures that a single worker will run the
callback (other workers trying to access the same value will wait).

=item 4.

a single worker runs the L3 callback (e.g. performs a database query)

=over


=item 1.

the callback succeeds and returns a value: the value is set in the
L2 cache, then in the L1 cache as well (as-is by default, or as
returned by C<l1_serializer> if specified).

=item 2.

the callback failed and returned C<nil, err>:
a. if C<resurrect_ttl> is specified, and if the stale value is still
available, it will be resurrected in the L2 cache.
b. otherwise, C<get()> returns C<nil, err>.

=back


=item 5.

other workers that were trying to access the same value but were waiting
are unlocked and fetch the value from the L2 cache (they do not run the L3
callback) and return it.


=back

Example:


    local mlcache = require "mlcache"
    
    local cache, err = mlcache.new("my_cache", "cache_shared_dict", {
        lru_size = 1000
    })
    if not cache then
        -- ...
    end
    
    local function fetch_user(db, user_id)
        local user, err = db:query_user(user_id)
        if err then
            -- in this case, get() will return `nil` + `err`
            return nil, err
        end
    
        return user -- table or nil
    end
    
    local user_id = 3
    local db = my_db_connection -- lua-resty-mysql instance
    
    local user, err = cache:get("users:" .. user_id, { ttl = 3600 }, fetch_user, db, user_id)
    if err then
        ngx.log(ngx.ERR, "could not retrieve user: ", err)
        return
    end
    
    -- `user` could be a table, but could also be `nil` (does not exist)
    -- regardless, it will be cached and subsequent calls to get() will
    -- return the cached value, for up to `ttl` or `neg_ttl`.
    if user then
        ngx.say("user exists: ", user.name)
    else
        ngx.say("user does not exists")
    end

This second example is the modification of the above one, in which we apply
some transformation to the retrieved C<user> record, and cache it via the
C<l1_serializer> callback:


    -- Our l1_serializer, called upon an L1 miss, when L2 or L3 return a hit.
    --
    -- Its signature accepts a single argument: the item as returned from
    -- an L2 hit. Therefore, this argument can never be `nil`. The result will be
    -- kept in the L1 LRU cache, but it cannot be `nil`.
    --
    -- This function can return `nil` and a string describing an error, which
    -- will be bubbled up to the `get()` call. It also runs in protected mode
    -- and will report any Lua error thrown.
    local function compile_custom_code(user_row)
        if user_row.custom_code ~= nil then
            local compiled, err = loadstring(user_row.custom_code)
            if not compiled then
                -- in this case, nothing will be stored in the cache (as if the L3
                -- callback failed). This means that if the same operation is
                -- attempted and the same data is returned, it will fail again.
                -- Depending on the situation it might not be desireable, and
                -- storing a default value in the L1 would be a better option.
                return nil, "failed to compile custom code: " .. err
            end
    
            user_row.custom_code = compiled
        end
    
        return user_row
    end
    
    local user, err = cache:get("users:" .. user_id,
                                { l1_serializer = compile_custom_code },
                                fetch_user, db, user_id)
    if err then
         ngx.log(ngx.ERR, "could not retrieve user: ", err)
         return
    end
    
    -- now we have a ready-to-call function which was only
    -- compiled once
    user.custom_code()




=head2 peek

B<syntax:> C<ttl, err, value = cache:peek(key)>

Peeks into the L2 (C<lua_shared_dict>) cache.

The first and only argument C<key> is a string, and it is the key to lookup.

This method returns C<nil> and a string describing the error upon failure.

Upon success, but if there is no such value for the queried C<key>, it returns
C<nil> as its first argument, and no error.

Upon success, and if there is such a value for the queried C<key>, it returns a
number indicating the remaining TTL of the cached value. The third returned
value in that case will be the cached value itself, for convenience.

This method is useful if you want to know whether a value is cached or not. A
value stored in the L2 cache is considered cache, regardless of whether or not
it is also set in the L1 cache of the worker. That is because the L1 cache is
too volatile (as its size unit is in a number of slots), and the L2 cache is
still several orders of magnitude faster than the L3 callback.

As its only intent is to take a "peek" into the cache to determine its warmth
for a given value, C<peek()> does not count as a query like L<get()>, and
does not set the value in the L1 cache.

Example:


    local mlcache = require "mlcache"
    
    local cache = mlcache.new("my_cache", "cache_shared_dict")
    
    local ttl, err, value = cache:peek("key")
    if err then
        ngx.log(ngx.ERR, "could not peek cache: ", err)
        return
    end
    
    ngx.say(ttl)   -- nil because `key` has no value yet
    ngx.say(value) -- nil
    
    -- cache the value
    
    cache:get("key", { ttl = 5 }, function() return "some value" end)
    
    -- wait 2 seconds
    
    ngx.sleep(2)
    
    local ttl, err, value = cache:peek("key")
    if err then
        ngx.log(ngx.ERR, "could not peek cache: ", err)
        return
    end
    
    ngx.say(ttl)   -- 3
    ngx.say(value) -- "some value"




=head2 set

B<syntax:> C<ok, err = cache:set(key, opts?, value)>

Unconditionally sets a value in the L2 cache and publish an event to other
workers so they can evict the value from their L1 cache.

The first argument C<key> is a string, and is the key under which to store the
value.

The second argument C<opts> is optional, and if provided, is identical to the
one of L<get()>.

The third argument C<value> is the value to cache, similar to the return value
of the L3 callback. Just like the callback's return value, it must be a Lua
scalar, a table, or C<nil>. If a C<l1_serializer> is provided either from the
constructor or in the C<opts> argument, it will be called with C<value> if
C<value> is not C<nil>.

On failure, this method returns C<nil> and a string describing the error.

On success, the first return value will be C<true>.

B<Note:> methods such as L<set()> and L<delete()> require that
other instances of mlcache (from other workers) evict the value from their
L1 (LRU) cache. Since OpenResty has currently no built-in mechanism for
inter-worker communication, this module relies on a polling mechanism via a
C<lua_shared_dict> shared memory zone to propagate inter-worker events. If
C<set()> or C<delete()> are called from a single worker, other workers' mlcache
instances bearing the same C<name> must call L<update()> before their
cache be requested during the next request, to make sure they evicted their L1
value, and that the L2 (fresh value) will be returned.

B<Note bis:> It is generally considered inefficient to call C<set()> on a hot
code path (such as in a request being served by OpenResty). Instead, one should
rely on L<get()> and its built-in mutex in the L3 callback. C<set()> is
better suited when called occasionally from a single worker, upon a particular
event that triggers a cached value to be updated, for example. Once C<set()>
updated the L2 cache with the fresh value, other workers will rely on
L<update()> to poll invalidation events. Calling C<get()> on those
other workers thus triggers an L1 miss, but the L2 access will hit the fresh
value.

B<See:> L<update()>




=head2 delete

B<syntax:> C<ok, err = cache:delete(key)>

Delete a value in the L2 cache and publish an event to other workers so they
can evict the value from their L1 cache.

The first and only argument C<key> is the string at which the value is stored.

On failure, this method returns C<nil> and a string describing the error.

On success, the first return value will be C<true>.

B<Note:> methods such as L<set()> and L<delete()> require that
other instances of mlcache (from other workers) evict the value from their
L1 (LRU) cache. Since OpenResty has currently no built-in mechanism for
inter-worker communication, this module relies on a polling mechanism via
a C<lua_shared_dict> shared memory zone to propagate inter-worker events. If
C<set()> or C<delete()> are called from a single worker, other workers' mlcache
instances bearing the same C<name> must call L<update()> before their
cache be requested during the next request, to make sure they evicted their L1
value, and that the L2 (fresh value) will be returned.

B<See:> L<update()>




=head2 purge

B<syntax:> C<ok, err = cache:purge(flush_expired?)>

Purge the content of the cache, in both the L1 and L2 levels. Then publishes
an event to other workers so they can purge their L1 cache as well.

This method recycles the lua-resty-lrucache instance, and calls
L<ngx.shared.DICT:flush_all|https://github.com/openresty/lua-nginx-module#ngxshareddictflush_all>
, so it can be rather expensive.

The first and only argument C<flush_expired> is optional, but if given C<true>,
this method will also call
L<ngx.shared.DICT:flush_expired|https://github.com/openresty/lua-nginx-module#ngxshareddictflush_expired>
(with no arguments). This is useful to release memory claimed by the L2 (shm)
cache if needed.

On failure, this method returns C<nil> and a string describing the error.

On success, the first return value will be C<true>.

B<Note:> this method, just like L<delete()>, requires that
other instances of mlcache (from other workers) purge their L1 (LRU) cache.
Since OpenResty has currently no built-in mechanism for inter-worker
communication, this module relies on a polling mechanism via a
C<lua_shared_dict> shared memory zone to propagate inter-worker events. If
this method is called from a single worker, other workers' mlcache instances
bearing the same C<name> must call L<update()> before their cache be
requested during the next request, to make sure they purged their L1 cache as
well.

B<See:> L<update()>




=head2 update

B<syntax:> C<ok, err = cache:update()>

Poll and execute pending cache invalidation events published by other workers.

Methods such as L<set()> and L<delete()> require that other
instances of mlcache (from other workers) evict the value from their L1 cache.
Since OpenResty has currently no built-in mechanism for inter-worker
communication, this module bundles an "off the shelf" IPC library to propagate
inter-worker events. If the bundled IPC library is used, the C<lua_shared_dict>
specified in the C<ipc_shm> option B<must not> be used by other actors than
mlcache itself.

This method allows a worker to update its L1 cache (by purging values
considered stale due to an other worker calling C<set()> or C<delete()>) before
processing a request.

A typical design pattern is to call C<update()> B<only once> on each request
processing. This allows your hot code paths to perform a single shm access in
the best case scenario: no invalidation events were received, all C<get()>
calls will hit in the L1 (LRU) cache. Only on a worst case scenario (C<n> values
were evicted by another worker) will C<get()> access the L2 or L3 cache C<n>
times. Subsequent requests will then hit the best case scenario again, because
C<get()> populated the L1 cache.

For example, if your workers make use of L<set()> or L<delete()>
anywhere in your application, call C<update()> at the entrance of your hot code
path, before using C<get()>:


    http {
        listen 9000;
    
        location / {
            content_by_lua_block {
                local cache = ... -- retrieve mlcache instance
    
                -- make sure L1 cache is evicted of stale values
                -- before calling get()
                local ok, err = cache:update()
                if not ok then
                    ngx.log(ngx.ERR, "failed to poll eviction events: ", err)
                    -- /!\ we might get stale data from get()
                end
    
                -- L1/L2/L3 lookup (best case: L1)
                local value, err = cache:get("key_1", nil, cb1)
                if err then
                    -- ...
                end
    
                -- L1/L2/L3 lookup (best case: L1)
                local other_value, err = cache:get(key_2", nil, cb2)
                if err then
                    -- ...
                end
    
                -- value and other_value are up-to-date because:
                -- either they were not stale and directly came from L1 (best case scenario)
                -- either they were stale and evicted from L1, and came from L2
                -- either they were not in L1 nor L2, and came from L3 (worst case scenario)
            }
        }
    
        location /delete {
            content_by_lua_block {
                local cache = ... -- retrieve mlcache instance
    
                -- delete some value
                local ok, err = cache:delete("key_1")
                if not ok then
                    ngx.log(ngx.ERR, "failed to delete value from cache: ", err)
                    return ngx.exit(500)
                end
    
                ngx.exit(204)
            }
        }
    
        location /set {
            content_by_lua_block {
                local cache = ... -- retrieve mlcache instance
    
                -- update some value
                local ok, err = cache:set("key_1", nil, 123)
                if not ok then
                    ngx.log(ngx.ERR, "failed to set value in cache: ", err)
                    return ngx.exit(500)
                end
    
                ngx.exit(200)
            }
        }
    }

B<Note:> you B<do not> need to call C<update()> to refresh your workers if
they never call C<set()>or C<delete()>. When workers only rely on C<get()>, values
expire naturally from the L1/L2 caches according to their TTL.

B<Note bis:> this library was built with the intent to use a better solution
for inter-worker communication as soon as one emerges. In future versions of
this library, if an IPC library can avoid the polling approach, so will this
library. C<update()> is only a necessary evil due to today's Nginx/OpenResty
"limitations". You can however use your own IPC library by use of the
C<opts.ipc> option when instantiating your mlcache.




=head1 Changelog


See L<CHANGELOG.md|CHANGELOG.md>.




=head1 License


Work licensed under the MIT License.



[lua-resty-lock]: https://github.com/openresty/lua-resty-lock
[lua-resty-lrucache]: https://github.com/openresty/lua-resty-lrucache
[lua_shared_dict]: https://github.com/openresty/lua-nginx-module#lua_shared_dict

[badge-travis-url]: https://travis-ci.org/thibaultcha/lua-resty-mlcache
[badge-travis-image]: https://travis-ci.org/thibaultcha/lua-resty-mlcache.svg?branch=master
